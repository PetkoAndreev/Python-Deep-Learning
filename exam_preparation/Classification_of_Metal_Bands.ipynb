{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, LSTM, Bidirectional, \\\n",
    "    Conv1D, MaxPool1D, Dense, Attention\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"./data/train/avenged_sevenfold/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BASE_DIR = \"./data/train/\"\n",
    "TEST_BASE_DIR = \"./data/test/\"\n",
    "\n",
    "N_CLASSES = 5\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_VOCABULARY_SIZE = 10000 #number unique words\n",
    "MAX_OUTPUT_LENGTH = 500 # kolko e dalga posledovatelnostta\n",
    "EMBEDDING_DIMENSIONS = 128\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.keras.utils.text_dataset_from_directory(TRAIN_BASE_DIR, BATCH_SIZE) \\\n",
    "    .cache() \\\n",
    "    . prefetch(buffer_size = tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.keras.utils.text_dataset_from_directory(TREST_BASE_DIR, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_dataset:\n",
    "    print(item[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for texts, labels in train_dataset:\n",
    "    print(texts[1])\n",
    "    print(labels[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(train_dataset.class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(\n",
    "    max_tokens = MAX_VOCABULARY_SIZE, \n",
    "    output_sequence_length = MAX_OUTPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(texts, labels):\n",
    "    return(texts)\n",
    "\n",
    "for i in train_dataset.map(get_texts).take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(train_dataset.map(get_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer([\"before the story begins is it such a sin\", \"this is a text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer(texts[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding(input_dim = MAX_VOCABULARY_SIZE, output_dim = EMBEDDING_DIMENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = mlflow.create_experiment(\"simplest possible LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.histogram(\"name\", [1,2,3]))\n",
    "\n",
    "texts_model = Sequential([\n",
    "    vectorizer,\n",
    "    Embedding(input_dim = vectorizer.vocabulary_size(), output_dim = EMBEDDING_DIMENSIONS),\n",
    "    LSTM(64, return_sequences = True),\n",
    "    Dense(len(train_dataset.class_names), activation = softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.predict([\"this is a test\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.predict([\"this is a test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.predict([\"this is a test\", \"this is another test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.compile(\n",
    "    loss = tf.keras.losses.SparseCategorical.Crossentropy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.compile(\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    optimizer = \"adam\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.compile(\n",
    "    loss = SparseCatgoricalCrossentropy(),\n",
    "    optimizer = Adam(learning_rate = LEARNING_RATE),\n",
    "    metrics = [\"acc\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id = experiment_id) as run:\n",
    "    mlfow.log_param(\"learning_rate\", LEARNING_RATE)\n",
    "    \n",
    "    run_id = mlflow.active_run().infor.run_id\n",
    "    #experiment_dir = mlflow.get_experiment(experiment_id).artifact_location\n",
    "    history = texts_model.fit(\n",
    "        train_dataset, \n",
    "        epochs = 10, \n",
    "        validation_data = test_dataset,\n",
    "        callbacks = [\n",
    "            TensorBoard(\n",
    "                log_dir = f\"./logs-(experiment_id)-(run_id)\",\n",
    "                write_graph = True),\n",
    "            ModelCheckpoint(\n",
    "                f\"./checkpoints-(experiment_id)-(run_id)\", \n",
    "                save_weights_only = False, \n",
    "                save_best_only = False)])\n",
    "    \n",
    "    mlfow.log_param(\"history\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = mlflow.create_experiment(\"BiDirectional LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "texts_model = Sequential([\n",
    "    vectorizer, \n",
    "    Embedding(input_dim = vectorizer.vocabulary_size(), output_dim = EMBEDDING_DIMENSIONS),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dense(len(train_dataset.class_names), activation = softmax)\n",
    "])\n",
    "\n",
    "texts_model.compile(\n",
    "    loss = SparseCatgoricalCrossentropy(),\n",
    "    optimizer = Adam(learning_rate = LEARNING_RATE),\n",
    "    metrics = [\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.layers[2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id = experiment_id) as run:\n",
    "    mlfow.log_param(\"learning_rate\", LEARNING_RATE)\n",
    "     mlfow.log_param(\"lstm_units\", LEARNING_RATE)\n",
    "    \n",
    "    run_id = mlflow.active_run().infor.run_id\n",
    "    #experiment_dir = mlflow.get_experiment(experiment_id).artifact_location\n",
    "    history = texts_model.fit(\n",
    "        train_dataset, \n",
    "        epochs = 10, \n",
    "        validation_data = test_dataset,\n",
    "        callbacks = [\n",
    "            TensorBoard(\n",
    "                log_dir = f\"./logs-(experiment_id)-(run_id)\",\n",
    "                write_graph = True)])\n",
    " #           ModelCheckpoint(\n",
    " #               f\"./checkpoints-(experiment_id)-(run_id)\", \n",
    " #               save_weights_only = False, \n",
    "#                save_best_only = False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = mlflow.create_experiment(\"Multi_layer BiDirectional LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "texts_model = Sequential([\n",
    "    vectorizer, \n",
    "    Embedding(input_dim = vectorizer.vocabulary_size(), output_dim = EMBEDDING_DIMENSIONS),\n",
    "    Bidirectional(LSTM(64, return_sequences = True)),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(N_CLASSES, activation = softmax)\n",
    "])\n",
    "\n",
    "texts_model.compile(\n",
    "    loss = SparseCatgoricalCrossentropy(),\n",
    "    optimizer = Adam(learning_rate = LEARNING_RATE),\n",
    "    metrics = [\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id = experiment_id) as run:\n",
    "    mlfow.log_param(\"learning_rate\", LEARNING_RATE)\n",
    "        \n",
    "    run_id = mlflow.active_run().infor.run_id\n",
    "    #experiment_dir = mlflow.get_experiment(experiment_id).artifact_location\n",
    "    history = texts_model.fit(\n",
    "        train_dataset, \n",
    "        epochs = 10, \n",
    "        validation_data = test_dataset,\n",
    "        callbacks = [\n",
    "            TensorBoard(\n",
    "                log_dir = f\"./logs-(experiment_id)-(run_id)\",\n",
    "                write_graph = True)])\n",
    " #           ModelCheckpoint(\n",
    " #               f\"./checkpoints-(experiment_id)-(run_id)\", \n",
    " #               save_weights_only = False, \n",
    "#                save_best_only = False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = mlflow.create_experiment(\"1D CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "texts_model = Sequential([\n",
    "    vectorizer, \n",
    "    Embedding(input_dim = vectorizer.vocabulary_size(), output_dim = EMBEDDING_DIMENSIONS),\n",
    "    Conv1d(64, 3, padding = \"same\", activation = \"relu\"),\n",
    "    Conv1d(64, 3, padding = \"same\", activation = \"relu\"),\n",
    "    MaxPool1D(),\n",
    "    Conv1d(128, 3, padding = \"same\", activation = \"relu\"),\n",
    "    \n",
    "    Dense(N_CLASSES, activation = softmax)\n",
    "])\n",
    "\n",
    "texts_model.compile(\n",
    "    loss = SparseCatgoricalCrossentropy(),\n",
    "    optimizer = Adam(learning_rate = LEARNING_RATE),\n",
    "    metrics = [\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = ((tf.keras.layers.Input((1, ))))\n",
    "\n",
    "b1 = block(embedding)\n",
    "b2 = block(b1)\n",
    "b3 = block(2)\n",
    "\n",
    "def block(input_1):\n",
    "    embedding = Embedding(input_dim = MAX_VOCABULARY_SIZE, output_dim = EMBEDDING_DIMENSIONS)(input_1)\n",
    "    conv1 = Conv1d(128, 3, padding = \"same\", activation = \"relu\")(embedding)\n",
    "\n",
    "    conv21 = Conv1d(128, 3, padding = \"same\", activation = \"relu\")(conv1)\n",
    "    conv22 = Conv1d(128, 3, padding = \"same\", activation = \"relu\")(conv1)\n",
    "    conv23 = Conv1d(128, 3, padding = \"same\", activation = \"relu\")(conv1)\n",
    "\n",
    "    concat = tf.keras.layers.Concatenate()(conv21, conv22, conv23)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = tf.keras.models.Model(inputs = embedding, outputs = concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id = experiment_id) as run:\n",
    "    mlfow.log_param(\"learning_rate\", LEARNING_RATE)\n",
    "        \n",
    "    run_id = mlflow.active_run().infor.run_id\n",
    "    #experiment_dir = mlflow.get_experiment(experiment_id).artifact_location\n",
    "    history = texts_model.fit(\n",
    "        train_dataset, \n",
    "        epochs = 10, \n",
    "        validation_data = test_dataset,\n",
    "        callbacks = [\n",
    "            TensorBoard(\n",
    "                log_dir = f\"./logs-(experiment_id)-(run_id)\",\n",
    "                write_graph = True)])\n",
    " #           ModelCheckpoint(\n",
    " #               f\"./checkpoints-(experiment_id)-(run_id)\", \n",
    " #               save_weights_only = False, \n",
    "#                save_best_only = False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = mlflow.create_experiment(\"Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "texts_model = Sequential([\n",
    "    vectorizer, \n",
    "    Embedding(input_dim = vectorizer.vocabulary_size(), output_dim = EMBEDDING_DIMENSIONS),\n",
    "    Bidirectional(LSTM(64, return_sequences = True)),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    tf.keras.layers.Attenion(),\n",
    "    Dense(N_CLASSES, activation = softmax)\n",
    "])\n",
    "\n",
    "texts_model.compile(\n",
    "    loss = SparseCatgoricalCrossentropy(),\n",
    "    optimizer = Adam(learning_rate = LEARNING_RATE),\n",
    "    metrics = [\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Sequential([\n",
    "    vectorizer, \n",
    "    Embedding(input_dim = vectorizer.vocabulary_size(), output_dim = EMBEDDING_DIMENSIONS),\n",
    "    Bidirectional(LSTM(64, return_sequences = True)),\n",
    "    Bidirectional(LSTM(64))\n",
    "])\n",
    "\n",
    "attention = Attention()([encoder.output, encoder.output])\n",
    "dense = Dense(5, activation = softmax)(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_id = 0\n",
    "line_vectorizer = TextVectorization(output_sequence_length = 20, vocabulary = vocabulary.get_vocabulary())\n",
    "line_dataset_train = tf.data.TextlineDataset([\", \"]) \\\n",
    "    .filter(lambda text: text != \"\") \\\n",
    "    .map(lambda text: (vectorizer(text, band_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in line_dataset_train.take(5):\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
